{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention层：\n",
    "输入是Q,K,V  \n",
    "`K.batch_dot`方法维度需要注意  \n",
    "Attention中各操作维度已标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#! -*- coding: utf-8 -*-\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "class Position_Embedding(Layer):\n",
    "    \n",
    "    def __init__(self, size=None, mode='sum', **kwargs):\n",
    "        self.size = size #必须为偶数\n",
    "        self.mode = mode\n",
    "        super(Position_Embedding, self).__init__(**kwargs)\n",
    "        \n",
    "    def call(self, x):\n",
    "        if (self.size == None) or (self.mode == 'sum'):\n",
    "            self.size = int(x.shape[-1])\n",
    "        batch_size,seq_len = K.shape(x)[0],K.shape(x)[1]\n",
    "        position_j = 1. / K.pow(10000., \\\n",
    "                                 2 * K.arange(self.size / 2, dtype='float32' \\\n",
    "                               ) / self.size)\n",
    "        position_j = K.expand_dims(position_j, 0)\n",
    "        position_i = K.cumsum(K.ones_like(x[:,:,0]), 1)-1 #K.arange不支持变长，只好用这种方法生成\n",
    "        position_i = K.expand_dims(position_i, 2)\n",
    "        position_ij = K.dot(position_i, position_j)\n",
    "        position_ij = K.concatenate([K.cos(position_ij), K.sin(position_ij)], 2)\n",
    "        if self.mode == 'sum':\n",
    "            return position_ij + x\n",
    "        elif self.mode == 'concat':\n",
    "            return K.concatenate([position_ij, x], 2)\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.mode == 'sum':\n",
    "            return input_shape\n",
    "        elif self.mode == 'concat':\n",
    "            return (input_shape[0], input_shape[1], input_shape[2]+self.size)\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "\n",
    "    def __init__(self, nb_head, size_per_head, mask_right=False, **kwargs):\n",
    "        self.nb_head = nb_head\n",
    "        self.size_per_head = size_per_head\n",
    "        self.output_dim = nb_head*size_per_head\n",
    "        self.mask_right = mask_right\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.WQ = self.add_weight(name='WQ', \n",
    "                                  shape=(input_shape[0][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WK = self.add_weight(name='WK', \n",
    "                                  shape=(input_shape[1][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WV = self.add_weight(name='WV', \n",
    "                                  shape=(input_shape[2][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "        \n",
    "    def Mask(self, inputs, seq_len, mode='mul'):\n",
    "        \"\"\"\n",
    "        inputs [B T E]\n",
    "        \"\"\"\n",
    "        if seq_len == None:\n",
    "            return inputs\n",
    "        else:\n",
    "            mask = K.one_hot(seq_len[:,0], K.shape(inputs)[1])\n",
    "            mask = 1 - K.cumsum(mask, 1)\n",
    "            for _ in range(len(inputs.shape)-2):\n",
    "                mask = K.expand_dims(mask, 2)\n",
    "            if mode == 'mul':\n",
    "                return inputs * mask\n",
    "            if mode == 'add':\n",
    "                return inputs - (1 - mask) * 1e12\n",
    "                \n",
    "    def call(self, x):\n",
    "        #如果只传入Q_seq,K_seq,V_seq，那么就不做Mask\n",
    "        #如果同时传入Q_seq,K_seq,V_seq,Q_len,V_len，那么对多余部分做Mask\n",
    "        if len(x) == 3:\n",
    "            Q_seq,K_seq,V_seq = x\n",
    "            Q_len,V_len = None,None\n",
    "        elif len(x) == 5:\n",
    "            Q_seq,K_seq,V_seq,Q_len,V_len = x\n",
    "        #对Q、K、V做线性变换\n",
    "        Q_seq = K.dot(Q_seq, self.WQ) # [B,T,E] * [E, E] = [B T E]\n",
    "        Q_seq = K.reshape(Q_seq, (-1, K.shape(Q_seq)[1], self.nb_head, self.size_per_head)) # [B T 8 16]\n",
    "        Q_seq = K.permute_dimensions(Q_seq, (0,2,1,3)) # [B 8 T 16]\n",
    "        K_seq = K.dot(K_seq, self.WK)\n",
    "        K_seq = K.reshape(K_seq, (-1, K.shape(K_seq)[1], self.nb_head, self.size_per_head))\n",
    "        K_seq = K.permute_dimensions(K_seq, (0,2,1,3))\n",
    "        V_seq = K.dot(V_seq, self.WV)\n",
    "        V_seq = K.reshape(V_seq, (-1, K.shape(V_seq)[1], self.nb_head, self.size_per_head))\n",
    "        V_seq = K.permute_dimensions(V_seq, (0,2,1,3))\n",
    "        #计算内积，然后mask，然后softmax\n",
    "        A = K.batch_dot(Q_seq, K_seq, axes=[3,3]) / self.size_per_head**0.5  # [B 8 T 16] batch_dot = [B 8 T T]\n",
    "        A = K.permute_dimensions(A, (0,3,2,1)) # [B T T 8]\n",
    "        A = self.Mask(A, V_len, 'add')\n",
    "        A = K.permute_dimensions(A, (0,3,2,1)) # [B 8 T T]\n",
    "        if self.mask_right:\n",
    "            ones = K.ones_like(A[:1, :1])\n",
    "            mask = (ones - K.tf.matrix_band_part(ones, -1, 0)) * 1e12\n",
    "            A = A - mask\n",
    "        A = K.softmax(A)\n",
    "        #输出并mask\n",
    "        O_seq = K.batch_dot(A, V_seq, axes=[3,2]) # [B 8 T T] * [B 8 T 16] = [B 8 T 16]\n",
    "        O_seq = K.permute_dimensions(O_seq, (0,2,1,3)) # [B T 8 16]\n",
    "        O_seq = K.reshape(O_seq, (-1, K.shape(O_seq)[1], self.output_dim)) # [B T E]\n",
    "        O_seq = self.Mask(O_seq, Q_len, 'mul')\n",
    "        return O_seq\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][1], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 添加自己实现的Attention(Q,V)\n",
    "使用权重$w$, $Q\\cdot W = A\\in \\mathbb{R}^{T\\times E}$，然后使用$A*V=O$  \n",
    "\n",
    "这时候是没有将维度缩进，如果需要实现multi-head，可以加一层映射，将multi-head的结果拼接后降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Attention(Layer):\n",
    "\n",
    "    def __init__(self, nb_head, size_per_head, mask_right=False, **kwargs):\n",
    "        self.nb_head = nb_head\n",
    "        self.size_per_head = size_per_head\n",
    "        self.output_dim = nb_head*size_per_head\n",
    "        self.mask_right = mask_right\n",
    "        super(My_Attention, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.WW = self.add_weight(name='WW', \n",
    "                                  shape=(128, 80),# W = [E, T]\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        super(My_Attention, self).build(input_shape)\n",
    "        \n",
    "    def Mask(self, inputs, seq_len, mode='mul'):\n",
    "        \"\"\"\n",
    "        inputs [B T E]\n",
    "        \"\"\"\n",
    "        if seq_len == None:\n",
    "            return inputs\n",
    "        else:\n",
    "            mask = K.one_hot(seq_len[:,0], K.shape(inputs)[1])\n",
    "            mask = 1 - K.cumsum(mask, 1)\n",
    "            for _ in range(len(inputs.shape)-2):\n",
    "                mask = K.expand_dims(mask, 2)\n",
    "            if mode == 'mul':\n",
    "                return inputs * mask\n",
    "            if mode == 'add':\n",
    "                return inputs - (1 - mask) * 1e12\n",
    "                \n",
    "    def call(self, x):\n",
    "        #如果只传入Q_seq,K_seq,V_seq，那么就不做Mask\n",
    "        #如果同时传入Q_seq,K_seq,V_seq,Q_len,V_len，那么对多余部分做Mask\n",
    "        if len(x) == 3:\n",
    "            Q_seq,K_seq,V_seq = x\n",
    "            Q_len,V_len = None,None\n",
    "        elif len(x) == 5:\n",
    "            Q_seq,K_seq,V_seq,Q_len,V_len = x\n",
    "        \n",
    "        A = K.dot(self.WW, Q_seq) # [E T] [B T E] = [E,B,E]\n",
    "        A = K.permute_dimensions(A, (1,0,2))\n",
    "        A = K.softmax(A)\n",
    "        O_seq = K.batch_dot(V_seq, A, axes=[2,2]) # []\n",
    "        return O_seq\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][1], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B=10\n",
    "T=5\n",
    "E=12\n",
    "inp = K.random_normal([B,T,E])\n",
    "w = K.random_normal([E,T])\n",
    "\n",
    "A = K.dot(w, inp) # [E,B,E]\n",
    "A = K.permute_dimensions(A, (1,0,2))\n",
    "A = K.softmax(A)\n",
    "o = K.batch_dot(inp, A, axes=[2,2])\n",
    "np.shape(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 55s 2ms/step - loss: 0.4146 - acc: 0.8086 - val_loss: 0.3517 - val_acc: 0.8460\n",
      "CPU times: user 4min 51s, sys: 12 s, total: 5min 3s\n",
      "Wall time: 55.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "S_inputs = Input(shape=(None,), dtype='int32') \n",
    "embeddings = Embedding(max_features, 128)(S_inputs) # embeddings [batch_size, max_lens, embedding_dim(128)]\n",
    "# embeddings = Position_Embedding()(embeddings) # 增加Position_Embedding能轻微提高准确率\n",
    "O_seq = Attention(8,16)([embeddings,embeddings,embeddings])\n",
    "O_seq = GlobalAveragePooling1D()(O_seq)\n",
    "O_seq = Dropout(0.5)(O_seq)\n",
    "outputs = Dense(1, activation='sigmoid')(O_seq)\n",
    "\n",
    "model = Model(inputs=S_inputs, outputs=outputs)\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.5207 - acc: 0.7543 - val_loss: 0.3902 - val_acc: 0.8295\n",
      "----------\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.3191 - acc: 0.8664 - val_loss: 0.3507 - val_acc: 0.8454\n",
      "----------\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.2584 - acc: 0.8963 - val_loss: 0.3505 - val_acc: 0.8468\n",
      "----------\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 49s 2ms/step - loss: 0.2190 - acc: 0.9148 - val_loss: 0.3693 - val_acc: 0.8415\n",
      "----------\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 49s 2ms/step - loss: 0.1877 - acc: 0.9301 - val_loss: 0.3915 - val_acc: 0.8366\n",
      "----------\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 50s 2ms/step - loss: 0.1627 - acc: 0.9405 - val_loss: 0.4238 - val_acc: 0.8328\n",
      "----------\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 50s 2ms/step - loss: 0.1406 - acc: 0.9503 - val_loss: 0.4623 - val_acc: 0.8276\n",
      "----------\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 51s 2ms/step - loss: 0.1190 - acc: 0.9588 - val_loss: 0.5078 - val_acc: 0.8232\n",
      "----------\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 49s 2ms/step - loss: 0.0957 - acc: 0.9681 - val_loss: 0.5635 - val_acc: 0.8173\n",
      "----------\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 49s 2ms/step - loss: 0.0730 - acc: 0.9778 - val_loss: 0.6404 - val_acc: 0.8147\n",
      "----------\n",
      "val_loss : 0.44497904971361163\n",
      "val_acc : 0.8315439999999998\n",
      "loss : 0.20958064949715136\n",
      "acc : 0.915724\n",
      "CPU times: user 31min 8s, sys: 1min 37s, total: 32min 45s\n",
      "Wall time: 8min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "S_inputs = Input(shape=(None,), dtype='int32')\n",
    "embeddings = Embedding(max_features, 128)(S_inputs) # embeddings [batch_size, max_lens, embedding_dim(128)]\n",
    "# embeddings = Position_Embedding()(embeddings) # 增加Position_Embedding能轻微提高准确率\n",
    "O_seq = My_Attention(8,16)([embeddings,embeddings,embeddings])\n",
    "O_seq = GlobalAveragePooling1D()(O_seq)\n",
    "O_seq = Dropout(0.5)(O_seq)\n",
    "outputs = Dense(1, activation='sigmoid')(O_seq)\n",
    "\n",
    "all_his = {}\n",
    "all_his['val_loss']=[]\n",
    "all_his['val_acc']=[]\n",
    "all_his['loss']=[]\n",
    "all_his['acc']=[]\n",
    "for i in range(10):\n",
    "    model = Model(inputs=S_inputs, outputs=outputs)\n",
    "    # try using different optimizers and different optimizer configs\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print('Train...')\n",
    "    his = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=1,\n",
    "              validation_data=(x_test, y_test))\n",
    "    \n",
    "    all_his['val_loss'].append(his.history['val_loss'])\n",
    "    all_his['val_acc'].append(his.history['val_acc'])\n",
    "    all_his['loss'].append(his.history['loss'])\n",
    "    all_his['acc'].append(his.history['acc'])\n",
    "    print('----------')\n",
    "for k,v in all_his.items():\n",
    "    print(k,':',np.mean(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 65s 3ms/step - loss: 0.4126 - acc: 0.8108 - val_loss: 0.3552 - val_acc: 0.8431\n",
      "----------\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 65s 3ms/step - loss: 0.2520 - acc: 0.8983 - val_loss: 0.3856 - val_acc: 0.8341\n",
      "----------\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 67s 3ms/step - loss: 0.1904 - acc: 0.9253 - val_loss: 0.4521 - val_acc: 0.8198\n",
      "----------\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 67s 3ms/step - loss: 0.1484 - acc: 0.9426 - val_loss: 0.5571 - val_acc: 0.8089\n",
      "----------\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 67s 3ms/step - loss: 0.1177 - acc: 0.9555 - val_loss: 0.6414 - val_acc: 0.7960\n",
      "----------\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 69s 3ms/step - loss: 0.0892 - acc: 0.9674 - val_loss: 0.8844 - val_acc: 0.7850\n",
      "----------\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 66s 3ms/step - loss: 0.0677 - acc: 0.9751 - val_loss: 0.9648 - val_acc: 0.7813\n",
      "----------\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 69s 3ms/step - loss: 0.0494 - acc: 0.9828 - val_loss: 1.0929 - val_acc: 0.7794\n",
      "----------\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 71s 3ms/step - loss: 0.0347 - acc: 0.9881 - val_loss: 1.3566 - val_acc: 0.7756\n",
      "----------\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 72s 3ms/step - loss: 0.0228 - acc: 0.9919 - val_loss: 1.6277 - val_acc: 0.7698\n",
      "----------\n",
      "val_loss : 0.831772185710907\n",
      "val_acc : 0.799308\n",
      "loss : 0.13848755510523247\n",
      "acc : 0.94378\n",
      "CPU times: user 45min 25s, sys: 2min 18s, total: 47min 44s\n",
      "Wall time: 11min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "S_inputs = Input(shape=(None,), dtype='int32')\n",
    "embeddings = Embedding(max_features, 128)(S_inputs) # embeddings [batch_size, max_lens, embedding_dim(128)]\n",
    "# embeddings = Position_Embedding()(embeddings) # 增加Position_Embedding能轻微提高准确率\n",
    "O_seq = Attention(8,16)([embeddings,embeddings,embeddings])\n",
    "O_seq = GlobalAveragePooling1D()(O_seq)\n",
    "O_seq = Dropout(0.5)(O_seq)\n",
    "outputs = Dense(1, activation='sigmoid')(O_seq)\n",
    "\n",
    "all_his = {}\n",
    "all_his['val_loss']=[]\n",
    "all_his['val_acc']=[]\n",
    "all_his['loss']=[]\n",
    "all_his['acc']=[]\n",
    "for i in range(10):\n",
    "    model = Model(inputs=S_inputs, outputs=outputs)\n",
    "    # try using different optimizers and different optimizer configs\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print('Train...')\n",
    "    his = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=1,\n",
    "              validation_data=(x_test, y_test))\n",
    "    \n",
    "    all_his['val_loss'].append(his.history['val_loss'])\n",
    "    all_his['val_acc'].append(his.history['val_acc'])\n",
    "    all_his['loss'].append(his.history['loss'])\n",
    "    all_his['acc'].append(his.history['acc'])\n",
    "    print('----------')\n",
    "for k,v in all_his.items():\n",
    "    print(k,':',np.mean(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2823871906841339"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((11*60+27) - (8*60+13)) / (11*60+27)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

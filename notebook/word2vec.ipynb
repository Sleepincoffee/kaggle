{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集1 (来源 宋艳青 博士处理后数据)\n",
    "## 原始数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = open(r'/mnt/d/Dataset/20newgroup/20ng-train-all-terms.txt', encoding='utf-8').readlines()\n",
    "data2 = open(r'/mnt/d/Dataset/20newgroup/20ng-test-all-terms.txt', encoding='utf-8').readlines()\n",
    "\n",
    "data = []\n",
    "[data.append(d.split('\\t')[1]) for d in data1]\n",
    "[data.append(d.split('\\t')[1]) for d in data2]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vector = vectorizer.fit_transform(data)\n",
    "print(\"训练数据共有{0}篇, 词汇计数为{1}个\".format(vector.shape[0], vector.shape[1]))\n",
    "\n",
    "vocabulary = vectorizer.vocabulary_\n",
    "VOCAB_SIZE = len(vocabulary)\n",
    "\n",
    "reverse_vocab = {v:k for k,v in vocabulary.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(data):\n",
    "    corpus = ''.join(data).split()\n",
    "    LEN = len(corpus)\n",
    "    \n",
    "    samples = []\n",
    "    for i,center_word in enumerate(corpus):\n",
    "        if i-2>=0:\n",
    "            samples.append((corpus[i-1],center_word))\n",
    "            samples.append((corpus[i-2],center_word))\n",
    "        if i+2<LEN:\n",
    "            samples.append((corpus[i+1],center_word))\n",
    "            samples.append((corpus[i+2],center_word))\n",
    "    return samples\n",
    "\n",
    "def trans_samples(data, vocab):\n",
    "    [(vocab.get(context_word), vocab.get(center_word)) for (context_word,center_word) in data]\n",
    "    \n",
    "    \n",
    "inputs = generate_samples(data)\n",
    "\n",
    "features = [(vocabulary.get(context_word), vocabulary.get(center_word)) for (context_word,center_word) in inputs]\n",
    "\n",
    "print(features[0])\n",
    "random.shuffle(features)\n",
    "print(features[0])\n",
    "\n",
    "\n",
    "samples_train = features[:19900000]\n",
    "samples_test = features[19900000:]\n",
    "\n",
    "x_train,y_train,x_test,y_test = [],[],[],[]\n",
    "\n",
    "for (x,y) in samples_train:\n",
    "    if x != None:\n",
    "        if y != None:\n",
    "            x_train.append(x)\n",
    "            y_train.append(y)\n",
    "\n",
    "for (x,y) in samples_test:\n",
    "    if x != None:\n",
    "        if y != None:\n",
    "            x_test.append(x)\n",
    "            y_test.append(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec 模型1  (自己实现)\n",
    "模型后使用数据2训练见最后\n",
    "[参考](https://blog.csdn.net/qq1483661204/article/details/78975847)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vocabulary)\n",
    "EMBED_SIZE = 100\n",
    "BATCH_SIZE = 64\n",
    "NUM_SAMPLE = 5\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "center_words = tf.placeholder(tf.int32, shape=[None])\n",
    "target_words = tf.placeholder(tf.int32, shape=[None, 1])\n",
    "\n",
    "encoder_matrix = tf.Variable(tf.random_uniform([VOCAB_SIZE, EMBED_SIZE], -1.0, 1.0))\n",
    "embeddings = tf.nn.embedding_lookup(encoder_matrix, center_words)\n",
    "\n",
    "decoder_matrix = tf.Variable(tf.truncated_normal([VOCAB_SIZE, EMBED_SIZE], stddev=1.0 / math.sqrt(EMBED_SIZE)))\n",
    "decoder_bias = tf.Variable(tf.zeros(VOCAB_SIZE))\n",
    "\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(weights=decoder_matrix,\n",
    "                                    biases=decoder_bias,\n",
    "                                    labels=target_words,\n",
    "                                    inputs=embeddings,\n",
    "                                    num_sampled=5,\n",
    "                                    num_classes=VOCAB_SIZE))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为了展示输出的效果，我们在训练的时候打印一些信息\n",
    "# 以下是求weight_emb的每个行的模长，但我们知道，其实他的每一行就对应一个词，我们把这些词对应的向量的模长求出来，\n",
    "# 然后将每个词对应的词向量变为单位向量,这样我们使用embedding_lookup取出词也是单位向量，那么计算余弦距离就可以\n",
    "# 直接矩阵相乘，得到我们所要计算词的余弦距离，然后我们在排序就可以取前几个最相似的词\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(encoder_matrix),axis=1,keep_dims=True))\n",
    "norm_embedd = encoder_matrix / norm\n",
    "# 随机选择8个词作为我们计算最相近的词\n",
    "# val_data = random.choices(list(vocabulary.keys()),k=5)\n",
    "val_data = ['geometric', 'monitor', 'mouse', 'linux', 'microsoft', 'engine', 'factory', 'storage', 'billion', 'article']\n",
    "val_int_data = tf.constant([vocabulary[i] for i in val_data],dtype=tf.int32)\n",
    "val_int_data_embed = tf.nn.embedding_lookup(encoder_matrix, val_int_data)\n",
    "similarity = tf.matmul(val_int_data_embed,tf.transpose(norm_embedd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    saver = tf.train.Saver()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter('./model/word2vec',session.graph)\n",
    "    \n",
    "    batch_size_all = len(x_train)//BATCH_SIZE\n",
    "    for i in range(batch_size_all+1):\n",
    "        x_batch = x_train[BATCH_SIZE*i:BATCH_SIZE*(i+1)]\n",
    "        y_batch = y_train[BATCH_SIZE*i:BATCH_SIZE*(i+1)]\n",
    "        y_batch = np.reshape(y_batch,[-1,1])\n",
    "\n",
    "        feed_dict = {center_words: x_batch, target_words: y_batch}\n",
    "        _, cur_loss = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        \n",
    "        if i % 200 == 0:\n",
    "            print(cur_loss)\n",
    "        if i % 2000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i,j in enumerate(val_data):\n",
    "                # 之所以sim取负号是因为为了从余弦距离最大到最小排列，因为argsort返回的是最小到达排列的索引\n",
    "                nearest_n = (-sim[i,:]).argsort()[1:5+1]\n",
    "                logg = 'Nearest to %s is :' % j\n",
    "                for ind,ner_int_word in enumerate(nearest_n):\n",
    "                    nearest_word = reverse_vocab[ner_int_word]\n",
    "                    logg = '%s  %s'%(logg,nearest_word)\n",
    "                print(logg) \n",
    "    save_path = saver.save(sess, \"./checkpoints/wrod2vec.ckpt\")\n",
    "    embed_mat = sess.run(norm_embedd)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型2 word2vec Tensorflow_model_github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emb_dim = 300\n",
    "vocab_size = len(vocabulary)\n",
    "batch_size = 128\n",
    "num_samples = 5\n",
    "epoches = 1000\n",
    "top_n = 10\n",
    "val_data = ['geometric', 'monitor', 'mouse', 'linux', 'microsoft', \n",
    "                           'engine', 'factory', 'storage', 'billion', 'article']\n",
    "\n",
    "\n",
    "def forward(vocab, counts):\n",
    "    \"\"\"Build the graph for the forward pass.\"\"\"\n",
    "    \n",
    "    examples = tf.placeholder(tf.int32, shape=[batch_size], name='input')\n",
    "    labels = tf.placeholder(tf.int32, shape=[batch_size, 1], name='label')\n",
    "    \n",
    "    # Embedding Weight [vocab_size, emb_dim]\n",
    "    init_width = 0.5 / emb_dim\n",
    "    emb = tf.Variable(\n",
    "        tf.random_uniform(\n",
    "            [vocab_size, emb_dim], -init_width),\n",
    "        name='emb')\n",
    "    \n",
    "    # Softmax Weight [vocab_size, emb_dim].Transposed.\n",
    "    sm_w_t = tf.Variable(\n",
    "        tf.zeros([vocab_size, emb_dim]),\n",
    "        name='sm_w_t')\n",
    "    \n",
    "    # Softmax bias [vocab_size]\n",
    "    sm_b = tf.Variable(tf.zeros([vocab_size]), name='sm_b')\n",
    "    \n",
    "    # NCE loss\n",
    "    labels_matrix = tf.reshape(\n",
    "        tf.cast(labels, dtype=tf.int64),\n",
    "        [batch_size, 1])\n",
    "    \n",
    "    # Negative sampling\n",
    "    sampled_ids, _, _ = (tf.nn.fixed_unigram_candidate_sampler(\n",
    "        true_classes=labels_matrix,\n",
    "        num_true=1,\n",
    "        num_sampled=num_samples,\n",
    "        unique=True,\n",
    "        range_max=vocab_size,\n",
    "        distortion=0.75,\n",
    "        unigrams=counts))\n",
    "    \n",
    "    # Embeddings for examples: [batch_size, emb_dim]\n",
    "    example_emb = tf.nn.embedding_lookup(emb, examples)\n",
    "    \n",
    "    # Weights for labels: [batch_size, emb_dim]\n",
    "    # 不需要计算所有的logistic 仅计算正确单词的logistic进行更新\n",
    "    true_w = tf.nn.embedding_lookup(sm_w_t, labels)\n",
    "    true_b = tf.nn.embedding_lookup(sm_b, labels)\n",
    "    \n",
    "    # Weights for sampled ids: [num_sampled, emb_dim]\n",
    "    sampled_w = tf.nn.embedding_lookup(sm_w_t, sampled_ids)\n",
    "    # Biases for sampled ids: [num_sampled, 1]\n",
    "    sampled_b = tf.nn.embedding_lookup(sm_b, sampled_ids)\n",
    "    \n",
    "    # True logits: [batch_size, 1]\n",
    "    true_logits = tf.reduce_sum(tf.multiply(example_emb, true_w), 1) + true_b\n",
    "    \n",
    "    # Sampled logits: [batch_size, numsampled]\n",
    "    sampled_b_vec = tf.reshape(sampled_b, [num_samples])\n",
    "    sampled_logits = tf.matmul(example_emb, \n",
    "                               sampled_w, \n",
    "                               transpose_b=True) + sampled_b_vec\n",
    "    \n",
    "    # Similarity option\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(emb), axis=1, keepdims=True))\n",
    "    norm_emb = emb / norm\n",
    "    \n",
    "    \n",
    "    val_int = tf.constant([vocab[word] for word in val_data], dtype=tf.int32)\n",
    "    val_emb = tf.nn.embedding_lookup(norm_emb, val_int)\n",
    "    similarity = tf.matmul(val_emb, tf.transpose(norm_emb))\n",
    "    \n",
    "    return examples, labels, true_logits, sampled_logits, similarity\n",
    "\n",
    "def nec_loss(true_logits, sampled_logits):\n",
    "    \"\"\"Build the graph for the NCE loss.\"\"\"\n",
    "\n",
    "    # cross-entropy (logits, labels)\n",
    "    true_xent = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        labels=tf.ones_like(true_logits), logits=true_logits)\n",
    "    \n",
    "    sampled_xent = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        labels=tf.zeros_like(sampled_logits), logits=sampled_logits)\n",
    "    \n",
    "    # NCE-loss is sum of the true and noise (sampled words)\n",
    "    # contributions, average over the batch.\n",
    "    nce_loss_tensor = (tf.reduce_sum(true_xent) + \n",
    "                       tf.reduce_sum(sampled_xent)) / batch_size\n",
    "    \n",
    "    return nce_loss_tensor\n",
    "\n",
    "def optimize(loss):\n",
    "    \"\"\"Build the graph to optimize the loss function.\"\"\"\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "    return optimizer\n",
    "\n",
    "def nearby():\n",
    "    pass\n",
    "\n",
    "def train(center_words, target_words, vocab, reverse_vocab, counts):\n",
    "    \"\"\"Build the graph for the full model.\"\"\"\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    examples, labels, true_logits, sampled_logits, similarity = forward(vocab, counts)\n",
    "    loss = nec_loss(true_logits, sampled_logits)\n",
    "    optimizer = optimize(loss)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        batch_all = len(center_words) // batch_size\n",
    "        print('Batch_all:', batch_all, 'Batch_size:', batch_size, 'Samples:', len(center_words))\n",
    "        for epoch in range(epoches):\n",
    "            for num in range(batch_all):\n",
    "                x_batch = center_words[num*batch_size: batch_size*(num+1)]\n",
    "                y_batch = target_words[num*batch_size: batch_size*(num+1)]\n",
    "\n",
    "                y_batch = np.array(y_batch).reshape(-1, 1)\n",
    "\n",
    "                _, l = sess.run([optimizer, loss], feed_dict={\n",
    "                    examples : x_batch,\n",
    "                    labels : y_batch\n",
    "                })\n",
    "                if num % 100 == 0:\n",
    "                    print('Epoch:',epoch,' Iter', num, 'loss:', l)\n",
    "                if num % 1000 == 0:\n",
    "                    sim = similarity.eval()\n",
    "                    for i,j in enumerate(val_data):\n",
    "                        nearest_n = (-sim[i, :]).argsort()[1:top_n+1]\n",
    "                        logg = 'Nearest to %s is :' % j\n",
    "                        for ind,ner_int_word in enumerate(nearest_n):\n",
    "                            nearest_word = reverse_vocab[ner_int_word]\n",
    "                            logg = '%s  %s'%(logg,nearest_word)\n",
    "                        print(logg)\n",
    "        \n",
    "        save_path = saver.save(session, \"./checkpoints/word2vec_model_20news.ckpt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集2 \n",
    "数据处理自`sklearn 20newsgroups`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"skip-gram模型数据\"\"\"\n",
    "\n",
    "\n",
    "def generate_samples(corpus, vocab, vocab_freq):\n",
    "    \"\"\"使用子采样生成数据\"\"\"\n",
    "    LEN = len(corpus)\n",
    "    rate = 0.001\n",
    "    samples = []\n",
    "    \n",
    "    for i,center_word in enumerate(corpus):\n",
    "        # 非词汇表词过滤\n",
    "        if i-2<0 or i+2>LEN-1 \\\n",
    "            or center_word is None \\\n",
    "            or center_word == vocab['.']:\n",
    "            continue\n",
    "        else:\n",
    "            condedate_words = [center_word, corpus[i-1], corpus[i-2], corpus[i+1], corpus[i+2]]\n",
    "            condedate_words = [word for word in condedate_words if word is not None]\n",
    "            freqs = np.array([vocab_freq[word] for word in condedate_words])\n",
    "            p_keeps = (np.sqrt(freqs/rate) + 1) * rate / freqs\n",
    "            p_keeps[p_keeps>1] = 1\n",
    "            \n",
    "            if  random.random() > p_keeps[0]: \n",
    "                # center_word 子采样\n",
    "                # print('center_word %d 舍弃' % center_word)\n",
    "                continue\n",
    "            else:\n",
    "                # target_word 子采样\n",
    "                sampled_words = [(center_word, condedate_words[i+1]) for i,p in enumerate(p_keeps[1:]) if random.random()<p]\n",
    "                samples.extend(sampled_words)\n",
    "                \n",
    "    return samples\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"加载数据\"\"\"\n",
    "    \n",
    "    newsgroup = fetch_20newsgroups(data_home='/mnt/d/Dataset/20newgroups/',\n",
    "                          subset='all',\n",
    "                          remove=('headers', 'footers', 'quotes'))\n",
    "    \n",
    "    corpus = ''.join(newsgroup.data).lower().split()\n",
    "    counter_corpus = Counter(corpus)\n",
    "\n",
    "    words_count = sorted(counter_corpus.items(), key=lambda kv:kv[1], reverse=True)\n",
    "    stop_words = '? ! @ # $ % ^ & * ( ) [ ] { } > < = - + ~ ` --- (i (or / ; ;\\' $1 |> \\\n",
    "                    --------- -------------------------------------------------------------------------- \\\n",
    "                    ========================= \\\n",
    "                    0 1 2 3 4 5 6 7 8 9 13 15 30 24 20 \"a\" tk> 95 45'\n",
    "    \n",
    "    index = 0\n",
    "    vocab_words, vocab, reverse_vocab, vocab_count, vocab_freq,= [],{},{},{},{}\n",
    "    for (k,v) in words_count:\n",
    "        if k in stop_words.split() or v < 15: continue\n",
    "        # 单词列表\n",
    "        vocab_words.append(k)\n",
    "        # 单词:id\n",
    "        vocab[k] = index\n",
    "        # id:单词\n",
    "        reverse_vocab[index] = k\n",
    "        # 单词:频次\n",
    "        vocab_count[k] = v\n",
    "        # 单词:频率\n",
    "        vocab_freq[index] = v/len(corpus)\n",
    "        index += 1\n",
    "\n",
    "    print('字典长度:', len(vocab.keys()))\n",
    "    print(vocab_words[:10])\n",
    "    \n",
    "    corpus_int = [vocab.get(word) for word in corpus]\n",
    "    # 非字典词语占比\n",
    "    print('非字典词语占比%.2f' % (corpus_int.count(None)/len(corpus_int)*100))\n",
    "    \n",
    "    samples = generate_samples(corpus_int, vocab, vocab_freq) # version2 实现子采样 传入原始语料(word list)而不是int\n",
    "    print('样本数量:',len(samples))\n",
    "    \n",
    "    return samples, list(vocab_count.values()), vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字典长度: 15282\n",
      "['the', 'to', 'of', 'a', 'and', 'in', 'is', 'i', 'that', 'for']\n",
      "非字典词语占比16.26\n",
      "样本数量: 5885881\n"
     ]
    }
   ],
   "source": [
    "samples,counts,vocabulary = load_data()\n",
    "reverse_vocab = {v:k for k,v in vocabulary.items()}\n",
    "center_words = [x for (x,y) in samples]\n",
    "target_words = [y for (x,y) in samples]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocabulary' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5ddb915f8e95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max>\\'ax>\\'ax>\\'ax>\\'ax>\\'ax>\\'ax>\\'ax>\\'ax>\\'ax>\\'ax>\\'ax>\\'ax>\\'ax>\\'ax>\\''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vocabulary' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "vocabulary['max>\\'ax>\\'ax>\\'ax>\\'ax>\\'ax>\\'ax>\\'ax>\\'ax>\\'ax>\\'ax>\\'ax>\\'ax>\\'ax>\\'ax>\\'']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 76539,\n",
       "         1: 61104,\n",
       "         2: 52556,\n",
       "         3: 53414,\n",
       "         4: 50200,\n",
       "         5: 45036,\n",
       "         6: 43316,\n",
       "         7: 46876,\n",
       "         8: 45292,\n",
       "         9: 38525,\n",
       "         10: 36548,\n",
       "         11: 38383,\n",
       "         12: 33423,\n",
       "         13: 34288,\n",
       "         14: 32579,\n",
       "         15: 33519,\n",
       "         16: 32385,\n",
       "         17: 33411,\n",
       "         18: 32387,\n",
       "         19: 31051,\n",
       "         20: 28554,\n",
       "         21: 28587,\n",
       "         22: 30011,\n",
       "         23: 28757,\n",
       "         24: 30463,\n",
       "         25: 26727,\n",
       "         26: 26809,\n",
       "         27: 26936,\n",
       "         28: 26933,\n",
       "         29: 27191,\n",
       "         30: 26220,\n",
       "         31: 25810,\n",
       "         32: 25531,\n",
       "         33: 25215,\n",
       "         34: 24119,\n",
       "         35: 25418,\n",
       "         36: 24319,\n",
       "         37: 25515,\n",
       "         38: 26356,\n",
       "         39: 23541,\n",
       "         40: 23309,\n",
       "         41: 24404,\n",
       "         42: 24018,\n",
       "         43: 22070,\n",
       "         44: 21242,\n",
       "         45: 19556,\n",
       "         46: 19258,\n",
       "         47: 20183,\n",
       "         48: 18508,\n",
       "         49: 18075,\n",
       "         50: 18819,\n",
       "         51: 17799,\n",
       "         52: 17710,\n",
       "         53: 16830,\n",
       "         54: 16386,\n",
       "         55: 15933,\n",
       "         56: 16060,\n",
       "         57: 15943,\n",
       "         58: 16363,\n",
       "         59: 15060,\n",
       "         60: 14169,\n",
       "         61: 14025,\n",
       "         62: 15998,\n",
       "         63: 14614,\n",
       "         64: 15904,\n",
       "         65: 15210,\n",
       "         66: 14638,\n",
       "         67: 15392,\n",
       "         68: 14084,\n",
       "         69: 14634,\n",
       "         70: 13456,\n",
       "         71: 13584,\n",
       "         72: 12553,\n",
       "         73: 11616,\n",
       "         74: 11319,\n",
       "         75: 11917,\n",
       "         76: 11484,\n",
       "         77: 10993,\n",
       "         78: 11721,\n",
       "         79: 11558,\n",
       "         80: 11382,\n",
       "         81: 10931,\n",
       "         82: 10439,\n",
       "         83: 10311,\n",
       "         84: 10097,\n",
       "         85: 9253,\n",
       "         86: 10046,\n",
       "         87: 9896,\n",
       "         88: 10178,\n",
       "         89: 8137,\n",
       "         90: 8628,\n",
       "         91: 9528,\n",
       "         92: 9274,\n",
       "         93: 8789,\n",
       "         94: 8664,\n",
       "         95: 8879,\n",
       "         96: 7982,\n",
       "         97: 8816,\n",
       "         98: 9144,\n",
       "         99: 8144,\n",
       "         100: 7869,\n",
       "         101: 7950,\n",
       "         102: 7265,\n",
       "         103: 7762,\n",
       "         104: 7598,\n",
       "         105: 7474,\n",
       "         106: 6637,\n",
       "         107: 7208,\n",
       "         108: 6760,\n",
       "         109: 7849,\n",
       "         110: 6845,\n",
       "         111: 7108,\n",
       "         112: 6234,\n",
       "         113: 7531,\n",
       "         114: 7467,\n",
       "         115: 6400,\n",
       "         116: 7517,\n",
       "         117: 6672,\n",
       "         118: 6726,\n",
       "         119: 5998,\n",
       "         120: 6286,\n",
       "         121: 6033,\n",
       "         122: 5879,\n",
       "         123: 5761,\n",
       "         124: 6006,\n",
       "         125: 5695,\n",
       "         126: 6150,\n",
       "         127: 5843,\n",
       "         128: 5590,\n",
       "         129: 6042,\n",
       "         130: 6488,\n",
       "         131: 5513,\n",
       "         132: 5193,\n",
       "         133: 6063,\n",
       "         134: 5074,\n",
       "         135: 5326,\n",
       "         136: 5605,\n",
       "         137: 5196,\n",
       "         138: 5183,\n",
       "         139: 4997,\n",
       "         140: 5380,\n",
       "         141: 4994,\n",
       "         142: 5227,\n",
       "         143: 4904,\n",
       "         144: 5017,\n",
       "         145: 5112,\n",
       "         146: 4483,\n",
       "         147: 4840,\n",
       "         148: 4808,\n",
       "         149: 4674,\n",
       "         150: 5247,\n",
       "         151: 4202,\n",
       "         152: 4793,\n",
       "         153: 4796,\n",
       "         154: 4745,\n",
       "         155: 4946,\n",
       "         156: 4785,\n",
       "         158: 4721,\n",
       "         159: 4613,\n",
       "         160: 4659,\n",
       "         161: 4759,\n",
       "         162: 4933,\n",
       "         163: 4257,\n",
       "         164: 3909,\n",
       "         165: 4714,\n",
       "         166: 4304,\n",
       "         167: 4249,\n",
       "         168: 4830,\n",
       "         169: 4622,\n",
       "         170: 4908,\n",
       "         171: 4580,\n",
       "         172: 4660,\n",
       "         173: 4405,\n",
       "         174: 4167,\n",
       "         175: 4530,\n",
       "         176: 4328,\n",
       "         177: 4363,\n",
       "         178: 4282,\n",
       "         179: 4340,\n",
       "         180: 3317,\n",
       "         181: 3896,\n",
       "         182: 3962,\n",
       "         183: 4233,\n",
       "         184: 4376,\n",
       "         185: 3674,\n",
       "         186: 3564,\n",
       "         187: 4119,\n",
       "         188: 4031,\n",
       "         189: 3878,\n",
       "         190: 3640,\n",
       "         191: 4267,\n",
       "         192: 3872,\n",
       "         193: 3754,\n",
       "         194: 3776,\n",
       "         195: 3921,\n",
       "         196: 3981,\n",
       "         197: 3477,\n",
       "         198: 3535,\n",
       "         199: 3697,\n",
       "         200: 3571,\n",
       "         201: 3610,\n",
       "         202: 3582,\n",
       "         203: 3814,\n",
       "         204: 3932,\n",
       "         205: 3984,\n",
       "         206: 3153,\n",
       "         207: 2882,\n",
       "         208: 3735,\n",
       "         209: 3256,\n",
       "         210: 3326,\n",
       "         211: 3456,\n",
       "         212: 2936,\n",
       "         213: 3178,\n",
       "         214: 3295,\n",
       "         215: 3119,\n",
       "         216: 3266,\n",
       "         217: 3442,\n",
       "         218: 3717,\n",
       "         219: 3122,\n",
       "         220: 3063,\n",
       "         221: 3308,\n",
       "         222: 3463,\n",
       "         223: 3271,\n",
       "         224: 3304,\n",
       "         225: 3306,\n",
       "         226: 3253,\n",
       "         227: 3242,\n",
       "         228: 2998,\n",
       "         229: 3238,\n",
       "         230: 2758,\n",
       "         231: 3061,\n",
       "         232: 2893,\n",
       "         233: 2494,\n",
       "         234: 3135,\n",
       "         235: 3031,\n",
       "         236: 2445,\n",
       "         237: 3005,\n",
       "         238: 2624,\n",
       "         239: 2769,\n",
       "         240: 2821,\n",
       "         241: 3016,\n",
       "         242: 2946,\n",
       "         243: 2241,\n",
       "         244: 2710,\n",
       "         245: 2745,\n",
       "         246: 3029,\n",
       "         247: 2784,\n",
       "         248: 2766,\n",
       "         249: 2762,\n",
       "         250: 2637,\n",
       "         251: 2360,\n",
       "         252: 2422,\n",
       "         253: 2969,\n",
       "         254: 2860,\n",
       "         255: 2482,\n",
       "         256: 2884,\n",
       "         257: 2705,\n",
       "         258: 2596,\n",
       "         259: 2482,\n",
       "         260: 2490,\n",
       "         261: 2599,\n",
       "         262: 2756,\n",
       "         263: 2887,\n",
       "         264: 2591,\n",
       "         265: 3087,\n",
       "         266: 2812,\n",
       "         267: 2483,\n",
       "         268: 2808,\n",
       "         269: 2719,\n",
       "         270: 2533,\n",
       "         271: 2725,\n",
       "         272: 2590,\n",
       "         273: 2609,\n",
       "         274: 2315,\n",
       "         275: 2464,\n",
       "         276: 2201,\n",
       "         277: 2745,\n",
       "         278: 2534,\n",
       "         279: 2836,\n",
       "         280: 2502,\n",
       "         281: 2725,\n",
       "         282: 2097,\n",
       "         283: 2544,\n",
       "         284: 2546,\n",
       "         285: 2744,\n",
       "         286: 2516,\n",
       "         287: 2083,\n",
       "         288: 2251,\n",
       "         289: 2232,\n",
       "         290: 2260,\n",
       "         291: 2546,\n",
       "         292: 2426,\n",
       "         293: 2496,\n",
       "         294: 2339,\n",
       "         295: 2348,\n",
       "         296: 2392,\n",
       "         297: 2371,\n",
       "         298: 2370,\n",
       "         299: 2406,\n",
       "         300: 2591,\n",
       "         301: 2282,\n",
       "         302: 2210,\n",
       "         303: 2559,\n",
       "         304: 2485,\n",
       "         305: 2207,\n",
       "         306: 2438,\n",
       "         307: 2124,\n",
       "         308: 2222,\n",
       "         309: 1983,\n",
       "         310: 2243,\n",
       "         311: 2196,\n",
       "         312: 2478,\n",
       "         313: 2234,\n",
       "         314: 2202,\n",
       "         315: 2349,\n",
       "         316: 2145,\n",
       "         317: 2170,\n",
       "         318: 2408,\n",
       "         319: 2103,\n",
       "         320: 2126,\n",
       "         321: 2212,\n",
       "         322: 2071,\n",
       "         323: 2040,\n",
       "         324: 2324,\n",
       "         325: 2110,\n",
       "         326: 2115,\n",
       "         327: 2430,\n",
       "         328: 2232,\n",
       "         329: 2204,\n",
       "         330: 2057,\n",
       "         331: 2326,\n",
       "         332: 2055,\n",
       "         333: 2129,\n",
       "         334: 2093,\n",
       "         335: 2239,\n",
       "         336: 2163,\n",
       "         337: 1930,\n",
       "         338: 2173,\n",
       "         339: 2194,\n",
       "         340: 1806,\n",
       "         341: 2021,\n",
       "         342: 2068,\n",
       "         343: 1802,\n",
       "         344: 2196,\n",
       "         345: 2196,\n",
       "         346: 1709,\n",
       "         347: 1917,\n",
       "         348: 2133,\n",
       "         349: 2056,\n",
       "         350: 1928,\n",
       "         351: 1994,\n",
       "         352: 2038,\n",
       "         353: 2093,\n",
       "         354: 2167,\n",
       "         355: 2279,\n",
       "         356: 1988,\n",
       "         357: 1933,\n",
       "         358: 1972,\n",
       "         359: 1957,\n",
       "         360: 2019,\n",
       "         361: 2242,\n",
       "         362: 2245,\n",
       "         363: 1893,\n",
       "         364: 1894,\n",
       "         365: 1994,\n",
       "         366: 2013,\n",
       "         367: 1885,\n",
       "         368: 1998,\n",
       "         369: 1822,\n",
       "         370: 2137,\n",
       "         371: 2073,\n",
       "         372: 2035,\n",
       "         373: 1948,\n",
       "         374: 2077,\n",
       "         375: 1818,\n",
       "         376: 1934,\n",
       "         377: 1944,\n",
       "         378: 1892,\n",
       "         379: 1947,\n",
       "         380: 1910,\n",
       "         381: 1925,\n",
       "         382: 1729,\n",
       "         383: 1799,\n",
       "         384: 1622,\n",
       "         385: 1848,\n",
       "         386: 1817,\n",
       "         387: 1860,\n",
       "         388: 1773,\n",
       "         389: 1895,\n",
       "         390: 1745,\n",
       "         391: 1999,\n",
       "         392: 1918,\n",
       "         393: 1650,\n",
       "         394: 1951,\n",
       "         395: 1847,\n",
       "         396: 1926,\n",
       "         397: 1945,\n",
       "         398: 1838,\n",
       "         399: 1899,\n",
       "         400: 1838,\n",
       "         401: 1912,\n",
       "         402: 1675,\n",
       "         403: 1815,\n",
       "         404: 2012,\n",
       "         405: 1736,\n",
       "         406: 1840,\n",
       "         407: 1808,\n",
       "         408: 1684,\n",
       "         409: 1776,\n",
       "         410: 1797,\n",
       "         411: 1776,\n",
       "         412: 1645,\n",
       "         413: 1636,\n",
       "         414: 1732,\n",
       "         415: 1556,\n",
       "         416: 1474,\n",
       "         417: 1727,\n",
       "         418: 1693,\n",
       "         419: 1646,\n",
       "         420: 1604,\n",
       "         421: 1780,\n",
       "         422: 1591,\n",
       "         423: 1658,\n",
       "         424: 1581,\n",
       "         425: 1626,\n",
       "         426: 1799,\n",
       "         427: 1712,\n",
       "         428: 1728,\n",
       "         429: 1427,\n",
       "         430: 1829,\n",
       "         431: 1684,\n",
       "         432: 1510,\n",
       "         433: 1485,\n",
       "         434: 1521,\n",
       "         435: 1774,\n",
       "         436: 1717,\n",
       "         437: 1631,\n",
       "         438: 1558,\n",
       "         439: 1415,\n",
       "         440: 1641,\n",
       "         441: 1577,\n",
       "         442: 1580,\n",
       "         443: 1456,\n",
       "         444: 1902,\n",
       "         445: 1696,\n",
       "         446: 1780,\n",
       "         447: 1473,\n",
       "         448: 1706,\n",
       "         449: 1828,\n",
       "         450: 1624,\n",
       "         451: 1522,\n",
       "         452: 1680,\n",
       "         453: 1596,\n",
       "         454: 1591,\n",
       "         455: 1606,\n",
       "         456: 1589,\n",
       "         457: 1410,\n",
       "         458: 1677,\n",
       "         459: 1510,\n",
       "         460: 1678,\n",
       "         461: 1359,\n",
       "         462: 1789,\n",
       "         463: 1664,\n",
       "         464: 1703,\n",
       "         465: 1803,\n",
       "         466: 1496,\n",
       "         467: 1338,\n",
       "         468: 1265,\n",
       "         469: 1433,\n",
       "         470: 1749,\n",
       "         471: 1538,\n",
       "         472: 1565,\n",
       "         473: 1475,\n",
       "         474: 1571,\n",
       "         475: 1452,\n",
       "         476: 1393,\n",
       "         477: 1527,\n",
       "         478: 1510,\n",
       "         479: 1728,\n",
       "         480: 1521,\n",
       "         481: 1396,\n",
       "         482: 1740,\n",
       "         483: 1530,\n",
       "         484: 1504,\n",
       "         485: 1465,\n",
       "         486: 166,\n",
       "         487: 1403,\n",
       "         488: 1535,\n",
       "         489: 1632,\n",
       "         490: 1526,\n",
       "         491: 1521,\n",
       "         492: 1489,\n",
       "         493: 1290,\n",
       "         494: 1482,\n",
       "         495: 1484,\n",
       "         496: 1402,\n",
       "         497: 1501,\n",
       "         498: 1408,\n",
       "         499: 1500,\n",
       "         500: 1576,\n",
       "         501: 1406,\n",
       "         502: 1549,\n",
       "         503: 1575,\n",
       "         504: 1437,\n",
       "         505: 1508,\n",
       "         506: 1442,\n",
       "         507: 1283,\n",
       "         508: 1437,\n",
       "         509: 1275,\n",
       "         510: 1465,\n",
       "         511: 1547,\n",
       "         512: 1483,\n",
       "         513: 1399,\n",
       "         514: 1599,\n",
       "         515: 1397,\n",
       "         516: 1503,\n",
       "         517: 1311,\n",
       "         518: 1531,\n",
       "         519: 1511,\n",
       "         520: 1369,\n",
       "         521: 1397,\n",
       "         522: 1433,\n",
       "         523: 1447,\n",
       "         524: 1516,\n",
       "         525: 1276,\n",
       "         526: 1508,\n",
       "         527: 1336,\n",
       "         528: 1538,\n",
       "         529: 1400,\n",
       "         530: 1322,\n",
       "         531: 1490,\n",
       "         532: 1208,\n",
       "         533: 1537,\n",
       "         534: 1178,\n",
       "         535: 1299,\n",
       "         536: 1441,\n",
       "         537: 1463,\n",
       "         538: 1347,\n",
       "         539: 1409,\n",
       "         540: 1061,\n",
       "         541: 1364,\n",
       "         542: 1380,\n",
       "         543: 1451,\n",
       "         544: 1237,\n",
       "         545: 1378,\n",
       "         546: 1283,\n",
       "         547: 1368,\n",
       "         548: 1326,\n",
       "         549: 1370,\n",
       "         550: 1340,\n",
       "         551: 1354,\n",
       "         552: 1279,\n",
       "         553: 1257,\n",
       "         554: 1361,\n",
       "         555: 918,\n",
       "         556: 1219,\n",
       "         557: 1218,\n",
       "         558: 1410,\n",
       "         559: 1343,\n",
       "         560: 1277,\n",
       "         561: 1075,\n",
       "         562: 1183,\n",
       "         563: 1275,\n",
       "         564: 1120,\n",
       "         565: 1258,\n",
       "         566: 1427,\n",
       "         567: 1357,\n",
       "         568: 1464,\n",
       "         569: 1178,\n",
       "         570: 1153,\n",
       "         571: 1175,\n",
       "         572: 1141,\n",
       "         573: 1329,\n",
       "         574: 1135,\n",
       "         575: 1190,\n",
       "         576: 1337,\n",
       "         577: 1472,\n",
       "         578: 1234,\n",
       "         579: 1328,\n",
       "         580: 1170,\n",
       "         581: 1265,\n",
       "         582: 1259,\n",
       "         583: 1319,\n",
       "         584: 1107,\n",
       "         585: 1267,\n",
       "         586: 1382,\n",
       "         587: 1362,\n",
       "         588: 1133,\n",
       "         589: 1376,\n",
       "         590: 1266,\n",
       "         591: 1142,\n",
       "         592: 1279,\n",
       "         593: 1257,\n",
       "         594: 1461,\n",
       "         595: 1325,\n",
       "         596: 1277,\n",
       "         597: 1210,\n",
       "         598: 1204,\n",
       "         599: 1262,\n",
       "         600: 1208,\n",
       "         601: 1083,\n",
       "         602: 1225,\n",
       "         603: 1245,\n",
       "         604: 1041,\n",
       "         605: 1230,\n",
       "         606: 1274,\n",
       "         607: 1253,\n",
       "         608: 1169,\n",
       "         609: 1092,\n",
       "         610: 1274,\n",
       "         611: 1052,\n",
       "         612: 1287,\n",
       "         613: 1148,\n",
       "         614: 1198,\n",
       "         615: 1267,\n",
       "         616: 1175,\n",
       "         617: 1156,\n",
       "         618: 1195,\n",
       "         619: 1226,\n",
       "         620: 1145,\n",
       "         621: 1152,\n",
       "         622: 1167,\n",
       "         623: 1155,\n",
       "         624: 1359,\n",
       "         625: 1275,\n",
       "         626: 1099,\n",
       "         627: 1076,\n",
       "         628: 1148,\n",
       "         629: 1367,\n",
       "         630: 1206,\n",
       "         631: 1101,\n",
       "         632: 1082,\n",
       "         633: 1184,\n",
       "         634: 1118,\n",
       "         635: 1341,\n",
       "         636: 1212,\n",
       "         637: 1093,\n",
       "         638: 1159,\n",
       "         639: 1156,\n",
       "         640: 1075,\n",
       "         641: 1092,\n",
       "         642: 1214,\n",
       "         643: 1101,\n",
       "         644: 1137,\n",
       "         645: 1055,\n",
       "         646: 1194,\n",
       "         647: 1168,\n",
       "         648: 1167,\n",
       "         649: 1122,\n",
       "         650: 1378,\n",
       "         651: 1142,\n",
       "         652: 1077,\n",
       "         653: 1223,\n",
       "         654: 968,\n",
       "         655: 1171,\n",
       "         656: 927,\n",
       "         657: 1270,\n",
       "         658: 1164,\n",
       "         659: 970,\n",
       "         660: 1194,\n",
       "         661: 1064,\n",
       "         662: 1016,\n",
       "         663: 1051,\n",
       "         664: 1080,\n",
       "         665: 1085,\n",
       "         666: 1156,\n",
       "         667: 1145,\n",
       "         668: 1138,\n",
       "         669: 1103,\n",
       "         670: 1274,\n",
       "         671: 1153,\n",
       "         672: 1105,\n",
       "         673: 1287,\n",
       "         674: 1186,\n",
       "         675: 1107,\n",
       "         676: 1086,\n",
       "         677: 1014,\n",
       "         678: 1062,\n",
       "         679: 1077,\n",
       "         680: 1146,\n",
       "         681: 1178,\n",
       "         682: 1164,\n",
       "         683: 1262,\n",
       "         684: 1158,\n",
       "         685: 1147,\n",
       "         686: 1171,\n",
       "         687: 1158,\n",
       "         688: 1099,\n",
       "         689: 1092,\n",
       "         690: 1112,\n",
       "         691: 1099,\n",
       "         692: 932,\n",
       "         693: 1064,\n",
       "         694: 966,\n",
       "         695: 1126,\n",
       "         696: 1119,\n",
       "         697: 1105,\n",
       "         698: 1146,\n",
       "         699: 993,\n",
       "         700: 996,\n",
       "         701: 1105,\n",
       "         702: 1059,\n",
       "         703: 1027,\n",
       "         704: 799,\n",
       "         705: 995,\n",
       "         706: 1051,\n",
       "         707: 964,\n",
       "         708: 1067,\n",
       "         709: 1092,\n",
       "         710: 1131,\n",
       "         711: 1085,\n",
       "         712: 1122,\n",
       "         713: 1012,\n",
       "         714: 1092,\n",
       "         715: 1118,\n",
       "         716: 1095,\n",
       "         717: 1079,\n",
       "         718: 980,\n",
       "         719: 1002,\n",
       "         720: 1052,\n",
       "         721: 1049,\n",
       "         722: 951,\n",
       "         723: 923,\n",
       "         724: 1181,\n",
       "         725: 956,\n",
       "         726: 960,\n",
       "         727: 1017,\n",
       "         728: 1066,\n",
       "         729: 1091,\n",
       "         730: 978,\n",
       "         731: 1094,\n",
       "         732: 951,\n",
       "         733: 999,\n",
       "         734: 1101,\n",
       "         735: 1082,\n",
       "         736: 1100,\n",
       "         737: 1006,\n",
       "         738: 1030,\n",
       "         739: 1100,\n",
       "         740: 939,\n",
       "         741: 1229,\n",
       "         742: 909,\n",
       "         743: 958,\n",
       "         744: 1048,\n",
       "         745: 1055,\n",
       "         746: 1185,\n",
       "         747: 982,\n",
       "         748: 896,\n",
       "         749: 1014,\n",
       "         750: 1055,\n",
       "         751: 1061,\n",
       "         752: 962,\n",
       "         753: 995,\n",
       "         754: 977,\n",
       "         755: 992,\n",
       "         756: 1064,\n",
       "         757: 913,\n",
       "         758: 1053,\n",
       "         759: 969,\n",
       "         760: 1063,\n",
       "         761: 1046,\n",
       "         762: 1196,\n",
       "         763: 986,\n",
       "         764: 907,\n",
       "         765: 1017,\n",
       "         766: 1029,\n",
       "         767: 954,\n",
       "         768: 883,\n",
       "         769: 1009,\n",
       "         770: 982,\n",
       "         771: 984,\n",
       "         772: 1029,\n",
       "         773: 1129,\n",
       "         774: 1011,\n",
       "         775: 959,\n",
       "         776: 902,\n",
       "         777: 953,\n",
       "         778: 948,\n",
       "         779: 710,\n",
       "         780: 958,\n",
       "         781: 1021,\n",
       "         782: 1058,\n",
       "         783: 929,\n",
       "         784: 656,\n",
       "         785: 1037,\n",
       "         786: 902,\n",
       "         787: 973,\n",
       "         788: 1092,\n",
       "         789: 862,\n",
       "         790: 1004,\n",
       "         791: 939,\n",
       "         792: 954,\n",
       "         793: 1082,\n",
       "         794: 999,\n",
       "         795: 1108,\n",
       "         796: 960,\n",
       "         797: 911,\n",
       "         798: 735,\n",
       "         799: 946,\n",
       "         800: 926,\n",
       "         801: 975,\n",
       "         802: 975,\n",
       "         803: 972,\n",
       "         804: 969,\n",
       "         805: 1035,\n",
       "         806: 959,\n",
       "         807: 977,\n",
       "         808: 883,\n",
       "         809: 884,\n",
       "         810: 842,\n",
       "         811: 924,\n",
       "         812: 842,\n",
       "         813: 993,\n",
       "         814: 1025,\n",
       "         815: 922,\n",
       "         816: 940,\n",
       "         817: 957,\n",
       "         818: 1015,\n",
       "         819: 1035,\n",
       "         820: 936,\n",
       "         821: 1002,\n",
       "         822: 973,\n",
       "         823: 934,\n",
       "         824: 884,\n",
       "         825: 915,\n",
       "         826: 840,\n",
       "         827: 930,\n",
       "         828: 1037,\n",
       "         829: 928,\n",
       "         830: 988,\n",
       "         831: 1050,\n",
       "         832: 1037,\n",
       "         833: 1003,\n",
       "         834: 956,\n",
       "         835: 808,\n",
       "         836: 966,\n",
       "         837: 945,\n",
       "         838: 813,\n",
       "         839: 975,\n",
       "         840: 800,\n",
       "         841: 889,\n",
       "         842: 919,\n",
       "         843: 883,\n",
       "         844: 929,\n",
       "         845: 805,\n",
       "         846: 1004,\n",
       "         847: 888,\n",
       "         848: 939,\n",
       "         849: 709,\n",
       "         850: 840,\n",
       "         851: 892,\n",
       "         852: 835,\n",
       "         853: 971,\n",
       "         854: 992,\n",
       "         855: 825,\n",
       "         856: 911,\n",
       "         857: 973,\n",
       "         858: 965,\n",
       "         859: 914,\n",
       "         860: 937,\n",
       "         861: 889,\n",
       "         862: 1032,\n",
       "         863: 936,\n",
       "         864: 775,\n",
       "         865: 906,\n",
       "         866: 874,\n",
       "         867: 938,\n",
       "         868: 909,\n",
       "         869: 854,\n",
       "         870: 827,\n",
       "         871: 899,\n",
       "         872: 864,\n",
       "         873: 856,\n",
       "         874: 915,\n",
       "         875: 982,\n",
       "         876: 877,\n",
       "         877: 946,\n",
       "         878: 867,\n",
       "         879: 893,\n",
       "         880: 808,\n",
       "         881: 911,\n",
       "         882: 924,\n",
       "         883: 811,\n",
       "         884: 818,\n",
       "         885: 900,\n",
       "         886: 847,\n",
       "         887: 834,\n",
       "         888: 905,\n",
       "         889: 925,\n",
       "         890: 809,\n",
       "         891: 863,\n",
       "         892: 856,\n",
       "         893: 806,\n",
       "         894: 846,\n",
       "         895: 843,\n",
       "         896: 941,\n",
       "         897: 926,\n",
       "         898: 867,\n",
       "         899: 896,\n",
       "         900: 862,\n",
       "         901: 763,\n",
       "         902: 808,\n",
       "         903: 810,\n",
       "         904: 863,\n",
       "         905: 861,\n",
       "         906: 906,\n",
       "         907: 937,\n",
       "         908: 901,\n",
       "         909: 658,\n",
       "         910: 898,\n",
       "         911: 851,\n",
       "         912: 846,\n",
       "         913: 873,\n",
       "         914: 993,\n",
       "         915: 888,\n",
       "         916: 962,\n",
       "         917: 815,\n",
       "         918: 864,\n",
       "         919: 772,\n",
       "         920: 1172,\n",
       "         921: 823,\n",
       "         922: 835,\n",
       "         923: 884,\n",
       "         924: 965,\n",
       "         925: 836,\n",
       "         926: 870,\n",
       "         927: 794,\n",
       "         928: 828,\n",
       "         929: 867,\n",
       "         930: 775,\n",
       "         931: 815,\n",
       "         932: 823,\n",
       "         933: 816,\n",
       "         934: 931,\n",
       "         935: 849,\n",
       "         936: 875,\n",
       "         937: 938,\n",
       "         938: 818,\n",
       "         939: 785,\n",
       "         940: 919,\n",
       "         941: 811,\n",
       "         942: 408,\n",
       "         943: 872,\n",
       "         944: 763,\n",
       "         945: 780,\n",
       "         946: 819,\n",
       "         947: 851,\n",
       "         948: 736,\n",
       "         949: 808,\n",
       "         950: 748,\n",
       "         951: 823,\n",
       "         952: 839,\n",
       "         953: 780,\n",
       "         954: 784,\n",
       "         955: 774,\n",
       "         956: 848,\n",
       "         957: 774,\n",
       "         958: 740,\n",
       "         959: 885,\n",
       "         960: 871,\n",
       "         961: 734,\n",
       "         962: 763,\n",
       "         963: 855,\n",
       "         964: 775,\n",
       "         965: 828,\n",
       "         966: 862,\n",
       "         967: 607,\n",
       "         968: 804,\n",
       "         969: 712,\n",
       "         970: 786,\n",
       "         971: 779,\n",
       "         972: 862,\n",
       "         973: 862,\n",
       "         974: 865,\n",
       "         975: 879,\n",
       "         976: 757,\n",
       "         977: 921,\n",
       "         978: 821,\n",
       "         979: 741,\n",
       "         980: 843,\n",
       "         981: 890,\n",
       "         982: 782,\n",
       "         983: 681,\n",
       "         984: 824,\n",
       "         985: 844,\n",
       "         986: 778,\n",
       "         987: 816,\n",
       "         988: 730,\n",
       "         989: 762,\n",
       "         990: 746,\n",
       "         991: 676,\n",
       "         992: 716,\n",
       "         993: 837,\n",
       "         994: 498,\n",
       "         995: 704,\n",
       "         996: 850,\n",
       "         997: 764,\n",
       "         998: 762,\n",
       "         999: 723,\n",
       "         1000: 782,\n",
       "         ...})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "['the', 'to', 'of', 'a', 'and', 'in', 'is', 'i', 'that', 'for']\n",
    "Counter({0: 76539,\n",
    "         1: 61104,\n",
    "         2: 52556,\n",
    "         3: 53414,\n",
    "         4: 50200,\n",
    "         5: 45036,\n",
    "         6: 43316,\n",
    "         7: 46876,\n",
    "         8: 45292,\n",
    "         9: 38525,\n",
    "         10: 36548,\n",
    "\"\"\"\n",
    "\n",
    "# 手动对高频词限制\n",
    "limit_samples = []\n",
    "for x,y in samples:\n",
    "    if x == 0 and random.random() < 0.65: continue\n",
    "    if x == 1 and random.random() < 0.3: continue\n",
    "    if x == 2 and random.random() < 0.3: continue\n",
    "    if x == 3 and random.random() < 0.3: continue\n",
    "    if x == 3 and random.random() < 0.1: continue\n",
    "    limit_samples.append((x,y)) \n",
    "    \n",
    "center_words = [x for (x,y) in limit_samples]\n",
    "target_words = [y for (x,y) in limit_samples]\n",
    "Counter(center_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-80c0fe81fdaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcenter_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "train(center_words, target_words, vocabulary, reverse_vocab, list(counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 第二个数据 第一个模型\n",
    "模型保存于`./checkpoints/wrod2vec.ckpt` (笔误)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = center_words \n",
    "y_train = target_words\n",
    "\n",
    "\n",
    "\n",
    "VOCAB_SIZE = 14525\n",
    "EMBED_SIZE = 100\n",
    "BATCH_SIZE = 128\n",
    "NUM_SAMPLE = 5\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "tf.reset_default_graph()\n",
    "center_words = tf.placeholder(tf.int32, shape=[None])\n",
    "target_words = tf.placeholder(tf.int32, shape=[None, 1])\n",
    "\n",
    "encoder_matrix = tf.Variable(tf.random_uniform([VOCAB_SIZE, EMBED_SIZE], -1.0, 1.0))\n",
    "embeddings = tf.nn.embedding_lookup(encoder_matrix, center_words)\n",
    "\n",
    "decoder_matrix = tf.Variable(tf.truncated_normal([VOCAB_SIZE, EMBED_SIZE], stddev=1.0 / math.sqrt(EMBED_SIZE)))\n",
    "decoder_bias = tf.Variable(tf.zeros(VOCAB_SIZE))\n",
    "\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(weights=decoder_matrix,\n",
    "                                    biases=decoder_bias,\n",
    "                                    labels=target_words,\n",
    "                                    inputs=embeddings,\n",
    "                                    num_sampled=5,\n",
    "                                    num_classes=VOCAB_SIZE))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss)\n",
    "\n",
    "\n",
    "\n",
    "# 为了展示输出的效果，我们在训练的时候打印一些信息\n",
    "# 以下是求weight_emb的每个行的模长，但我们知道，其实他的每一行就对应一个词，我们把这些词对应的向量的模长求出来，\n",
    "# 然后将每个词对应的词向量变为单位向量,这样我们使用embedding_lookup取出词也是单位向量，那么计算余弦距离就可以\n",
    "# 直接矩阵相乘，得到我们所要计算词的余弦距离，然后我们在排序就可以取前几个最相似的词\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(encoder_matrix),axis=1,keep_dims=True))\n",
    "norm_embedd = encoder_matrix / norm\n",
    "# 随机选择8个词作为我们计算最相近的词\n",
    "val_data = random.choices(list(vocabulary.keys()),k=5)\n",
    "val_int_data = tf.constant([vocabulary[i] for i in val_data],dtype=tf.int32)\n",
    "val_int_data_embed = tf.nn.embedding_lookup(encoder_matrix, val_int_data)\n",
    "similarity = tf.matmul(val_int_data_embed,tf.transpose(norm_embedd))\n",
    "\n",
    "\n",
    "with tf.Session() as session:\n",
    "    saver = tf.train.Saver()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter('./model/word2vec',session.graph)\n",
    "    \n",
    "    batch_size_all = len(x_train)//BATCH_SIZE\n",
    "    \n",
    "    for e in range(10):\n",
    "        for i in range(batch_size_all+1):\n",
    "            x_batch = x_train[BATCH_SIZE*i:BATCH_SIZE*(i+1)]\n",
    "            y_batch = y_train[BATCH_SIZE*i:BATCH_SIZE*(i+1)]\n",
    "            y_batch = np.reshape(y_batch,[-1,1])\n",
    "\n",
    "            feed_dict = {center_words: x_batch, target_words: y_batch}\n",
    "            _, cur_loss = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "\n",
    "            if i % 2000 == 0:\n",
    "                print('Epoch:',e,' index:', i, ' loss:', cur_loss)\n",
    "            if i % 10000 == 0:\n",
    "                sim = similarity.eval()\n",
    "                for i,j in enumerate(val_data):\n",
    "                    # 之所以sim取负号是因为为了从余弦距离最大到最小排列，因为argsort返回的是最小到达排列的索引\n",
    "                    nearest_n = (-sim[i,:]).argsort()[1:5+1]\n",
    "                    logg = 'Nearest to %s is :' % j\n",
    "                    for ind,ner_int_word in enumerate(nearest_n):\n",
    "                        nearest_word = reverse_vocab[ner_int_word]\n",
    "                        logg = '%s  %s'%(logg,nearest_word)\n",
    "                    print(logg) \n",
    "    save_path = saver.save(session, \"./checkpoints/wrod2vec.ckpt\")\n",
    "    embed_mat = session.run(norm_embedd)\n",
    "    writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

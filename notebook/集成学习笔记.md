
# 集成学习笔记

参考资料：
- [使用sklearn进行集成学习——理论](http://www.cnblogs.com/jasonfreak/p/5657196.html)
- [为什么说bagging是减少variance，而boosting是减少bias?](https://www.zhihu.com/question/26760839)
- [GBDT](https://zhuanlan.zhihu.com/p/29765582)

Tip:**Bootstrap采样**  
又称为自助法，是有放回抽取。
步骤：
1. 采用重抽样技术从原始样本中抽取一定数量（自己给定）的样本，此过程允许重复抽样。
2. 根据抽出的样本计算给定的统计量T。
3. 重复上述N次（一般大于1000），得到N个统计量T。
4. 计算上述N个统计量T的样本方差，得到统计量的方差。

集成学习框架：**bagging**，**boosting**和**stacking**

## 1 偏差和方差
广义的偏差（bias）描述的是预测值和真实值之间的差异，方差（variance）描述距的是预测值作为随机变量的离散程度。方差也可以用来描述模型的波动性。
![@偏差和方差](http://pa54oihmf.bkt.clouddn.com/18-6-29/37729920.jpg)

### 1.1 模型的偏差和方差
模型的偏差比较简单，可以理解为模型在训练集上的准确率。  
模型的方差可以理解为模型的结构差异，不需要定量分析。


一般认为：**方差越大的模型越容易过拟合**  

集成学习框架中的基模型是弱模型，通常表示为弱模型是偏差高（在训练集上准确度低）方差小（防止过拟合能力强）的模型。但是，并不是所有集成学习框架中的基模型都是弱模型。bagging和stacking中的基模型为强模型（偏差低方差高），boosting中的基模型为弱模型。

由于Bagging和Boosting的基模型都是线性组成的，可以得到：

$$\begin{array}{l}{\mathrm{E}(F)=E\left(\sum_{i}^{m} \gamma_{i} * f_{i}\right)} \\ {=\sum_{i}^{m} \gamma_{i} * E\left(f_{i}\right)} \\ {=\gamma * \sum_{i}^{m} E\left(f_{i}\right)}\end{array}$$

$$\begin{array}{l}{\operatorname{Var}(F)=\operatorname{Var}\left(\sum_{i}^{m} \gamma_{i} * f_{i}\right)} \\ {=\operatorname{cov}\left(\sum_{i}^{m} \gamma_{i} * f_{i} . \sum_{i}^{m} \gamma_{i} * f_{i}\right)} \\ {=\sum_{i}^{m} \gamma_{i}^{2} * \operatorname{Var}\left(f_{i}\right)+\sum_{i}^{m} \sum_{j \neq i}^{m} 2 * \rho * \gamma_{i} * \gamma_{j} * \sqrt{\operatorname{Var}\left(f_{i}\right)} * \sqrt{\operatorname{Var}\left(f_{j}\right)}} \\ {=m^{2} * \gamma^{2} * \sigma^{2} * \rho+m * \gamma^{2} * \sigma^{2} *(1-\rho)}\end{array}$$


### 1.2 bagging的偏差和方差
对于bagging来说，每个基模型的权重等于1/m且期望近似相等(Boostrap采样方式)，偏差与方差可化简为：
$$\begin{array}{l}{\mathrm{E}(F)=\gamma \cdot \sum_{i}^{m} E\left(f_{i}\right)} \\ {=\frac{1}{m} * m * \mu} \\ {=\mu}\end{array}$$

$$\begin{array}{l}{\operatorname{Var}(F)=m^{2} * \gamma^{2} * \sigma^{2} * \rho+m * \gamma^{2} * \sigma^{2} *(1-\rho)} \\ {=m^{2} * \frac{1}{m^{2}} * \sigma^{2} * \rho+m * \frac{1}{m^{2}} *(1-\rho)} \\ {=\sigma^{2} * \rho+\frac{\sigma^{2} *(1-\rho)}{m}}\end{array}$$

根据上式我们可以看到，整体模型的期望近似于基模型的期望，即整体模型的偏差和基模型的偏差近似。同时，整体模型的方差小于等于基模型的方差(当相关性为1时取等号)，随着基模型数（m）的增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高。但是，模型的准确度一定会无限逼近于1吗？并不一定，当基模型数增加到一定程度时，方差公式第二项的改变对整体方差的作用很小，防止过拟合的能力达到极限，这便是准确度的极限了。另外，这也表明bagging中的基模型一定要为**强模型**，否则就会导致整体模型的偏差度低，即准确度低。


**随机森林**(Random Forest)是典型的基于bagging框架的模型，其在bagging的基础上，进一步降低了模型的方差。随机森林中基模型是树模型，在树的内部节点分裂过程中，不再是将所有特征，而是随机抽样一部分特征纳入分裂的候选项。这样一来，基模型之间的相关性降低，从而在方差公式中，第一项显著减少，第二项稍微增加，整体方差仍是减少。

### 1.3 boosting的偏差和方差
对于boosting来说，基模型的训练集抽样是强相关的，那么模型的相关系数近似等于1，故我们也可以针对boosting化简公式为：
$$\mathrm{E}(F)=\gamma * \sum_{i}^{m} E\left(f_{i}\right)$$

$$\begin{array}{l}{\operatorname{Var}(F)=m^{2} * \gamma^{2} * \sigma^{2} * \rho+m * \gamma^{2} * \sigma^{2} *(1-\rho)} \\ {=m^{2} * \gamma^{2} * \sigma^{2} * 1+m * \gamma^{2} * \sigma^{2} *(1-1)} \\ {=m^{2} * \gamma^{2} * \sigma^{2}}\end{array}$$

通过观察可以发现模型的方差与基模型方差相关，与基模型数量无关，因此，boosting框架中的基模型必须为弱模型(高偏差，低方差)。随着基模型的增加，模型整体期望与真实值越来越接近，从而准确率提高。所以在训练过程中，模型在训练集的准确率上越来越高，一定程度后，方差变大，准确率下降。

基于boosting框架的Gradient Tree Boosting模型中基模型也为树模型，同**随机森林**(Random Forrest)，我们也可以对特征进行随机抽样来使基模型间的相关性降低，从而达到减少方差的效果。

### 1.4 小结
- 使用模型的偏差和方差来描述其在训练集上的准确度和防止过拟合的能力
- 对于bagging来说，整体模型的偏差和基模型近似，随着训练的进行，整体模型的方差降低
- 对于boosting来说，整体模型的初始偏差较高，方差较低，随着训练的进行，整体模型的偏差降低（虽然也不幸地伴随着方差增高），当训练过度时，因方差增高，整体模型的准确度反而降低
- 整体模型的偏差和方差与基模型的偏差和方差息息相关

## 2. Boosting 
**Adaboost**只能应用于二分类问题{-1,1}，侧向于降低偏差。
模型表示：$$H(x)=\sum_{t=1}^T\alpha_t\cdot h_t(x)$$ 
损失函数指数函数：$$l_{exp}(H | D) = E_{x\sim D}[e^{-f(x)H(x)}]$$  
修改$\alpha_t$使$l_{exp}$最小，得到(同时也是**权重更新公式**)：$$\alpha_t=\frac{1}{2}\{\frac{1-\epsilon_t}{\epsilon}\}$$
因而得到理想的基学习器 $h_t(x)=\underset{h}{argmin} E_{x\sim D}[]$  
- 弱分类器应基于数据集$D_t$
- 针对$D_t$的分类误差应小于0.5

> 在训练的每一轮，根据样本分布的每个训练样本重赋权重，对无法接收带权样本的学习算法，通过**重采样**来处理

## 3. Bagging （Bootstrap Aggregating）
**Bagging**可以不经过修改就可以适用于多分类和回归问题，侧向于降低方差。
**Random Forest**:在以决策树为基学习器构建bagging集成基础上，进一步在训练过程中引入了随机属性选择。在全部$d$个属性中选取k个子属性。**推荐取值**$k=log_2d$.
RF一般优于Bagging。

## 4. 组合策略（aggregating）
- 数值(回归)：简单平均，加权平均
- 分类：投票，加权投票(硬投票[0,1]，软投票[概率])

## 5. Gradient Boosting
讲的不够详细  

基于boosting框架的整体模型可以用线性组成式来描述，其中h[i](x)为基模型与其权值的乘积：

$$\mathrm{F}(x)=\sum_{i}^{m} h_{i}(x)$$

上式表明模型整体的训练目标是使预测值F(x)与y逼近，即每个基模型预测与其对应部分的真实值。现在将改成为迭代式：

$$F^{i}(x)=F^{i-1}(x)+h_{i}(x)$$

这样一来，每一轮迭代，只需要一个基模型问题：使使F[i](x)逼近真实值y。

### 5.1 拟合残差
使$F[i](x)$逼近真实值，其实就是使$h[i](x)$逼近真实值和上一轮迭代的预测值$F[i-1](x)$之差，即残差（$y-F[i-1](x)$）。

研究者发现，残差其实是最小均方损失函数的关于预测值的反向梯度：

$$-\frac{\partial\left(\frac{1}{2} *\left(y-F_{i-1}(x)\right)^{2}\right)}{\partial F(x)}=y-F_{i-1}(x)$$

也就是说，若$F[i-1](x)$加上拟合了反向梯度的$h[i](x)$得到$F[i](x)$，该值可能将导致平方差损失函数降低，预测的准确度提高！

### 5.2 拟合反向梯度

损失函数：指数函数

$$L\left(y_{j}, F^{i-1}\left(x_{j}\right)\right)=e^{\left(-y_{j} z^{*-1}\left(x_{j}\right)\right)}$$

## 6. GBDT

残差就是负梯度：

损失函数：$L(y, F(x))=\frac{1}{2}(y-F(X))^{2}$  
最小化目标：$J=\frac{1}{2} \sum_{0}^{n}\left(y_{i}-F\left(x_{i}\right)\right)^{2}$

损失函数的一阶导数：$\frac{\partial J}{\partial F\left(x_{i}\right)}=\frac{\partial \sum_{i} L\left(y_{i}, F\left(x_{i}\right)\right)}{\partial F\left(x_{i}\right)}=\frac{\partial L\left(y_{i}, F\left(x_{i}\right)\right)}{\partial F\left(x_{i}\right)}=F\left(x_{i}\right)-y_{i}$

所以残差就是负梯度:
$y_{i}-F\left(x_{i}\right)=-\frac{\partial J}{\partial F\left(x_{i}\right)}$

但是GBDT对异常点很敏感，所以在回归类问题会选择使用绝对损失或者huber损失函数来代替平方损失函数：
- Absolute loss(more robust to outliers): L(y, F)=|y-F|
- Huber loss: $L(y, F)=\left\{\begin{array}{ll}{\frac{1}{2}(y-F)^{2}} & {|y-F| \leq \delta} \\ {\delta(|y-F|-\delta / 2)} & {|y-F|>\delta}\end{array}\right.$

### 6.1 Boosting加法模型
gbdt模型可以认为是是由k个基模型组成的一个加法运算式：

$$\hat{y}_{i}=\sum_{k=1}^{K} f_{k}\left(x_{i}\right), f_{k} \in F \tag{0}$$

其中F是指所有基模型组成的函数空间。则在n个样本上的一般化损失函数：
$L=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}\right)$

算法的损失函数可以替代理解模型的偏差，最小化损失函数就是最小化模型偏差，因此添加正则项来降低模型方差。

$O b j=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}\right)+\sum_{k=1}^{K} \Omega\left(f_{k}\right)$

其中 $\Omega$ 代表了基模型的复杂度，若基模型是树模型，则树的深度、叶子节点数等指标可以反应树的复杂程度。

以第t步模型的拟合为例，模型对第i个样本$x_i$的拟合为:

$\hat{y}_{i}^{t}=\hat{y}_{i}^{t-1}+f_{t}\left(x_{i}\right)$

其中 $f_t(x_i)$ 就是添加的基模型，此时，目标函数就可以写成：

$$\begin{aligned} O b j^{(t)} &=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{t}\right)+\sum_{i=i}^{t} \Omega\left(f_{i}\right) \\ &=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{t-1}+f_{t}\left(x_{i}\right)\right)+\Omega\left(f_{t}\right)+\text {constant } \end{aligned} \tag{1}$$

即此时最优化目标函数，就相当于求得了 $f_t(x_i)$ 。

### 6.2 GBDT目标函数
GBDT是一阶导数，xgboost是二阶导。泰勒公式,二阶导(一阶导更简单些，以二阶为例)如下：
$$f(x+\Delta x) \approx f(x)+f^{\prime}(x) \Delta x+\frac{1}{2} f^{\prime \prime}(x) \Delta x^{2} \tag{2}$$

那么在等式（1）中，我们把 $\hat{y}_i^{t-1}$ 看成是等式（2）中的x， $f_t(x_i)$ 看成是 $\Delta x$ ，因此等式（1）可以写成：

$O b j^{(t)}=\sum_{i=1}^{n}\left[l\left(y_{i}, \hat{y}_{i}^{t-1}\right)+g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\Omega\left(f_{t}\right)+\text {constant } \tag{3}$

其中 $g_{i}$ 为损失函数的一阶导， $h_{i}$ 为损失函数的二阶导，注意这里的导是对 $\hat{y}_i^{t-1}$ 求导。以平方损失函数为例:

- $g_{i}=\partial_{\hat{y}^{t-1}}\left(\hat{y}^{t-1}-y_{i}\right)^{2}=2\left(\hat{y}^{t-1}-y_{i}\right)$
- $h_{i}=\partial_{\hat{y}^{t-1}}^{2}\left(\hat{y}^{t-1}-y_{i}\right)^{2}=2$

由于在第t步 $\hat{y}_i^{t-1}$ 其实是一个已知的值，所以  $l(y_i, \hat{y}_i^{t-1})$ 是一个常数，其对函数优化不会产生影响，因此，等式（3）可以写成：

$$O b j^{(t)} \approx \sum_{i=1}^{n}\left[g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\Omega\left(f_{t}\right) \tag{4}$$

所以我么只要求出每一步损失函数的一阶和二阶导的值（由于前一步的 $\hat{y}^{t-1}$ 是已知的，所以这两个值就是常数）代入等式4，然后最优化目标函数，就可以得到每一步的 $f(x)$ ，最后根据加法模型得到一个整体模型。

### 6.3 GBDT算法
- [GBDT](https://zhuanlan.zhihu.com/p/29765582)

## 7. 树模型小结
### ID3
信息增益=信息熵-条件熵
信息熵的计算公式：$$Ent(D)= - \sum_{k}^{\mid y\mid} p_k\cdot log_2p_k$$
条件熵的计算公式：$$\sum_{v=1}^{V}=\frac{\mid D_v\mid}{\mid D \mid}Ent(D^v)$$
信息增益计算公式：$$Gain(D,a) = Ent(D) - \sum_{v=1}^{V}\frac{\mid D_v\mid}{\mid D \mid}Ent(D^v)$$
不足：
1. 不能处理连续性变量
2. 不能处理缺失值
3. 分裂变量倾向于类别较多e变量
4. 过拟合
5. 对取值数目较多的属性有偏好

### C4.5
优势：
1. 可以处理连续值，如有$a_1,a_2,...a_m$设立$m-1$个切分点，取两点平均值作为阈值。
2. 能处理一定的缺失值(未细看)
3. 使用信息增益率
信息增益率公式：$$Gain_ratio(D,a) = \frac{Gain(D,a)}{IV(a)}$$ $$IV(a) = -\sum_{v=1}^{V}\frac{\mid D^v\mid}{\mid D\mid}log_2\frac{\mid D^v\mid}{\mid D\mid}$$
不足：
1. C4.5 生成的是多叉树
2. 采用熵模型和增益率(对数运算)，结合1.导致计算量过大
3. 不能应用于回归问题
4. 对取值数目较少的属性有偏好

### CART(Classify and Regression Tree)
重要：
- 引入了**Gini**指数
- 二叉树

CART因使用最优二值切分点，而不是ID3和C4.5直接选择最优特征。
Gini指数公式：$$Gini(D)=\sum_{k=1}^{\mid y\mid}\sum_{k'\neq k}p_kp_{k'}=1-\sum_{k=1}^{k}{p_k}^2$$
与信息熵相对应，属性$a$的基尼指数定义：$$Gain_index(D,a)=\sum_{v=1}^{V}\frac{\mid D^v\mid}{\mid D\mid}Gini(D^v)$$  
由于Gini指数计算方式简单，故运行速度快。

### 随机森林
假设有N个样本，M个特征。
建立决策树步骤：
1. 利用bootstrap从N各样本抽取N个样本(带重复)作为根节点待分裂样本。
2. 在每个待分裂节点上，从M各特征中随机抽取m各特征(推荐 $log_2M$),利用Gini/信息增益进行分裂
3. 重复2直到阈值：树深或叶子节点数量 完成一个决策树的建立
4. 重复生成决策树，直到阈值限制。

随机森林采用CART是由于其二叉树特性及Gini计算简单考虑。  
> ** 随机森林的feature_importance不靠谱 ** 常用两种计算方式：1.mean decrease impurity 2. mean decrease accuracy [代码证明](http://127.0.0.1:8888/notebooks/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97.ipynb#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%9A%84%E7%89%B9%E5%BE%81%E9%87%8D%E8%A6%81%E6%80%A7%E6%B5%8B%E8%AF%95)

### BDT/GBDT/xgboost
BDT：Booting Decision Tree,使用多棵CART回归树通过残差进行学习。  
GBDT：Gradient Boosting Decision Tree 梯度提升决策树  
xgboost 是一种系统实现，并不是简单改进算法。


